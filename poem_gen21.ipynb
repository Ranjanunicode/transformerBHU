{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1u_ceUR-i9GPJIVXjvManHsB-pkQ9_dS2",
      "authorship_tag": "ABX9TyMZAqxSAGO0Wa6ulvR6fbJ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjanunicode/transformerBHU/blob/main/poem_gen21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4IpoJ6cEekF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "2gxKr2lMEi0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03765871-d9e9-4af3-c126-558afb25f00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7yZ2QFtvEljl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9flI7_GvEn1O",
        "outputId": "0e649228-d7fb-4e23-bb75-27cd3d9e3281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = open(\"/content/drive/MyDrive/hindi_text/sample_dhoopbahuthai_1.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        "# print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "id": "Nypymr3H5dwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8m7xW5WEo6x",
        "outputId": "eb63ac29-ee1d-476f-f402-d72f2ea5c13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Acf0v1ErPS",
        "outputId": "d6318689-2a70-44c9-e1b7-ad5f47423c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic7TlP1rExRU",
        "outputId": "58b9efde-5689-409e-aa24-0c810f7d2cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example_texts = [\"राहत\", \"चहकारें]\"]\n",
        "\n",
        "# # TODO 1\n",
        "# chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "# chars"
      ],
      "metadata": {
        "id": "cBMTJ4pH5rC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "cI62OE1iEuXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boUULW7dEz9w",
        "outputId": "b38ecec6-6bfa-4754-e4b6-60718f5cdc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "zTbH6WbRE3Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXRDwmRaE8Pb",
        "outputId": "48a528f4-3779-4c17-9226-2a50ea131f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si5BjuQ5E-dB",
        "outputId": "913e2b15-f16c-4531-a1ba-d7eb7f88f08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "l4d8AuqFFAam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfm7SWMCFDE3",
        "outputId": "201e756e-0b9a-4aa0-9291-57cc515bd4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "dKTx71RZFL3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFem5rrvFPAG",
        "outputId": "b781d707-4520-4360-bee9-61c386f4ebd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "stuexILOE5VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J0ibnruFboe",
        "outputId": "e64dc3fe-931a-4582-c794-5564830fd8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FZ2eYYFFeZw",
        "outputId": "ab479a51-1deb-45e3-f120-82819fe7f1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "yi7yDHZ6Fnnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFvprz9xFlBz",
        "outputId": "64b8c9e7-bd80-439a-d57c-bc39a55c2170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "mSHI6k6XFqWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGpMn8JFF2xO",
        "outputId": "29a51059-056a-4ea4-baeb-2dc662818826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM1nbWlcF3IT",
        "outputId": "93a76eb0-def3-41ae-e589-3b8271052092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "LPd4TfpgF5ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "NNMfFXX4F8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "fw7DEd2DF--q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBEmvytbGCDo",
        "outputId": "0c66ed79-62f2-413b-bde1-86bc94112acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fQ75gUIGGrb",
        "outputId": "c53b0c78-3c85-4f4d-c79f-0341f2c21780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "cOndDVCQGJP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TjDBlDAGMyB",
        "outputId": "93194203-a290-413d-f7e0-5a5a1d2b0d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5, 43, 32, 30, 19, 49, 42, 61,  8, 52, 29, 24, 23, 53, 60,  9, 60,\n",
              "       27, 61, 18,  3,  1, 27, 63,  3, 25,  7, 18,  6, 28,  3, 28, 62, 39,\n",
              "       56, 58, 65, 44,  7, 27,  9, 52, 59, 62, 65, 18,  3, 25, 20, 49, 31,\n",
              "       26, 36, 14, 30, 52, 53, 14, 49, 10, 38, 56, 27, 58, 37, 21, 23, 41,\n",
              "       11,  5, 65, 41, 26, 29, 18, 15, 33, 27, 53, 35,  0, 59, 14,  8, 21,\n",
              "       19, 55,  7, 52, 48, 37, 36, 26,  3, 13, 23, 30, 11, 22, 41])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV1tFYIgGPQQ",
        "outputId": "5730e6f7-4fe9-4e6f-eec6-c70f5a4ab024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'm,\\nMore than thou hast, and with it joy thy life;\\nSo as thou livest in peace, die free from strife:\\n'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"&dSQFjcv-mPKJnu.uNvE!\\nNx!L,E'O!OwZqsze,N.mtwzE!LGjRMWAQmnAj3YqNsXHJb:&zbMPEBTNnV[UNK]tA-HFp,miXWM!?JQ:Ib\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - add a loss function here\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "cBvvpMXGGSlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7QsedL9GV-a",
        "outputId": "dc947a0e-d9f3-435e-b292-09f09f5cb88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189863, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STvd5Br1GcPu",
        "outputId": "87c77ce7-2166-4889-9835-4dee30dcf386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.013756"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "b3H8Z8uLGeNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "34Ub1shUGgBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "5pMKtmUcGiHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUkoBKNmGk2f",
        "outputId": "50844476-1c02-49c3-b0ef-5f75422a12a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 17s 57ms/step - loss: 2.7525\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 2.0090\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7275\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 14s 59ms/step - loss: 1.5629\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4614\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.3922\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.3390\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.2938\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 14s 59ms/step - loss: 1.2523\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 14s 56ms/step - loss: 1.2144\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.1752\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.1347\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0931\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0488\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0017\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.9525\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9007\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8484\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7972\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7462\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.7016\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.6581\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.6189\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.5867\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.5597\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.5340\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5174\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.5003\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4846\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4724\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4630\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4548\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.4451\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4413\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4389\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4322\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4246\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 0.4257\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.4252\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.4221\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4207\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4167\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.4155\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4142\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4112\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4133\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.4111\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4170\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4182\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "YQBUzbprGnm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "gXzvzNPjGq_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp3qGtJJGtTE",
        "outputId": "071fce00-ab7a-4082-9921-1aacee1e32b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The write is not worse than Judnis,' treasure of again;\n",
            "Over the devil's man beg root as he,\n",
            "But with his last lives banish him, so surprised on himself\n",
            "and the first affection. Play'd and knell\n",
            "Opportuned he that is low in outring\n",
            "That they have quenches here: I am palabel, I'll give thee.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Seit an our brother, take away for virtuous\n",
            "Is nothing left are their pettitood and weep\n",
            "Till I can make my oath in heaven forbid!\n",
            "Weigh it be so, it shall become of me?\n",
            "If fortune I have deserved no pity.\n",
            "The air I love a man to death! a braggart, are you life\n",
            "On the first knave me of this father.\n",
            "\n",
            "JULIET:\n",
            "No, praise me of thy country, and a villain,\n",
            "Why stand'st thy unhated.\n",
            "Now, where's my mother bring us to?\n",
            "\n",
            "PROSPERO:\n",
            "Thou trill, a happy is to die.\n",
            "\n",
            "Provost:\n",
            "He's within; Let me have it musicion.\n",
            "\n",
            "WARWICK:\n",
            "O pardon, go, pray thee from thine ornamed\n",
            "in their ancee thornemning or a rat-up to life:\n",
            "I pray God Cleapen's easy to be avoided;' bush'd,\n",
            "And in his piece from heaven find \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.374945878982544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaErc37eGwdw",
        "outputId": "49618133-96c9-41ff-a784-e573678e68b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe wish doth Edward here shall take her die.\\n\\nEXETER:\\nHer further than the rest will second came unto;\\nAnd say 'What is't your husband and her father is brought:\\nThe everless done far from heart since I saw my cousin\\nUpon his brother, take it for his death:\\nMake haste, or sure, my tender-hearted kept must war\\nThurhy tyranny before her fortice;\\nFor either wear impeachenties' entertaighments be a kingdom's word,\\nThough and burne it, then in men: how cheer his pleasure\\nOf fouls of this direful murder o' the maid.\\n\\nHASTINGS:\\nGramers, your sister, then, have you not for me this allowing,\\nI tell thee what: get thee in her fellow?\\n\\nTYRREL:\\nPray you, put up; or rather for her keeper was surfeits, it\\nwithin me respect.\\n\\nSICINIUS:\\nHe were returned their royal rebels from his grave:\\nRind from none so were they from age to age,\\nMy vice and trumpets and tread treasury\\nOf two sleeping with our comfort.\\n\\nAUFIDIUS:\\nI wish,\\nWe may, then know you are bound to visit him.\\n\\nKING RICHARD II:\\nFarewell, my \"\n",
            " b\"ROMEO:\\nThere is the Duke of York, as it were, in a villain,\\nI would prother to the fire, a man of fine.\\n\\nPETRUCHIO:\\nThy news, good dury heartily.\\nI would thou wert so hard as high allayary block:\\nI do request your knogls to these gentlemen,\\nThe clouded blunt patience, wife, have man\\nAnon home that I have to hope against their hands.\\nAs if I were best knock your mar in't, and, in not behold\\nThe creatures late to the tail for thy death,\\nVincetino created in my under gaid intemping face,\\nThan bear him hither, he eats up in as every day\\nSay their gentle bause frightly finding her grave interr'd;\\nHid heart is broke. Go make me rive,\\nAnd dare the entrails of the feast,\\nHe bears by durst not have thy love: they didst best of\\nit.\\n\\nCOMINIUS:\\nYes, as thou livest in peace; but that's a botto\\nMy bonds of something home in,--battled from ninetern,\\nO, how 'tis post that think it honest, and I know her virtues.\\n\\nWARWICK:\\nO passion lets the truth of it.\\n\\nLUCIO:\\nI warrant your honour.\\n\\nDUKE VINCENTIO:\\nSold I\"\n",
            " b\"ROMEO:\\nThen was your sister should so much a feast.\\n\\nPERDITA:\\nI'll not made her accused with prattle--\\nAs an an eye recueed, I would not forget.\\n\\nLEONTES:\\nWas weep no great authority: if I\\nHad servants true about you; Lord Rivers, and when\\nwe set betied than heir with 'Whoop, do coins upon thee,\\nHaving dened in-maid such friends with ready. Baheman is your crown,\\nWe should have lived in vain with idle business\\nIn fortune respected with the fire, a piteous hand,\\nAnd spite of special damned steep day to try by\\nHer upon child of his tormentings: this\\nno matter seeing in his tread; so I revived\\nAnd vapeless care for ere this day's just,\\nAnd thou art worthy of the sin will hear no mean.\\nHath always glad to sea? woment we may,\\nDress of his fair consent by I do won,\\nNor cap without so blots: men are beholding his,\\nLest that thy grace unhold them, with our bloods\\nTo time can was no one unweeping reigning.\\n\\nDUKE OF YORK:\\nTut, dub's inscrice: I do leave his grace in bestia.\\n\\nGLOUCESTER:\\nClarence, star\"\n",
            " b\"ROMEO:\\nThe wisd's chreated sick that one of those venom admiting\\nShall be protacled of her friends:\\nTo be spoke frem our marchbands for thy leisure.\\nIs was my wedded down and know him;\\nFor me, is Aufidius, indeed is the wound which rememently\\nThrough all the waters; but thy son Petruchly would do not\\nconsent to your regard: 'tis wed more walls,\\nUnder 'I and left no farther than a months allow.\\n\\nFirst Huntsman:\\nMy lord and sovereign liege I have in him.\\n\\nQUEEN MARGARET:\\nBut thou art well; this he should smile this mile!\\nYou'ry son--Titus summing but above, man?\\nWhat do you need, what do it?\\n\\nBENVOLIO:\\nO must be none; Pitthen from the cares of their\\neyes; for as the all-seeing summer prince\\nEven to the wives bend their birth, and go along with us,\\nOr shumbland so love dastly counsel to his bright and traitor's uncless,\\nAnd could not now upon the weary humour.\\nWithin ten to one his breast to me.\\nAnd wrong you sour,\\nYou have a daughter shore, where is he? hold thee?\\n\\nFRIAR JOHN:\\nGood sir, I humb\"\n",
            " b\"ROMEO:\\nThe widow Dilan Marcius, are you live\\nTo ease thy entertake of death, to offer the bosom\\nThen being anointed length in virtue,\\nThat will mane off the worst of doom of daughter's war.\\n\\nFRIAR LAURENCE:\\nRomeo, be crountious lander, and love them to\\nThe treasures of his fiery steed,\\nOr how to lose his birthriage, fair, with his\\nMantish'd from my ordinance sad task him flows,\\nAnd say 'Wis vine and spent an oak.\\n\\nALINE:\\nNot at our business, behold their very coins\\nOf times for Claudio's joint, our king, my sun\\nAnd patient up and fled for this offence?\\n\\nGREMIO:\\nTwo, now breathed time be burnt; or,\\nof such as leisure in my love all fast.\\nNo wife to have her hand to thee.\\n\\nJULIET:\\nO Glouce the father, can so us. come, go with me;\\nLet us be barbed against the windoo.\\nThat which I would please me to my sinful soul;\\nFor inducts him love was grown to meet me fare\\nTo yield with patience, and we offend you at a rap-have willingly\\nBy under your royal greysy yet:\\nPreport a wife, were stopp'd up penite\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.502708911895752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJbqBvEJGznM",
        "outputId": "a211ced7-bb6d-4614-f912-9b8e56873f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f0ae0186470>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4FTK1dFG2XF",
        "outputId": "35fc0067-da3b-401d-b347-4146a2a01c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The wifes of hour little babeful womb\n",
            "Expose these thousand father, or thy mother\n",
            "Netters to ship a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced: Customized Training The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use tf.GradientTape to track the gradients. You can learn more about this approach by reading the eager execution guide.\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "Execute the model and calculate the loss under a tf.GradientTape. Calculate the updates and apply them to the model using the optimizer."
      ],
      "metadata": {
        "id": "mi1Ij8l6G-CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "7xLcyPWyG4rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "JNXmoYHpG7Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "id": "wph0M-YgHCSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5bQCai_HEF4",
        "outputId": "cafbc3f4-c80f-478d-823c-707cda0e507a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 15s 58ms/step - loss: 2.6853\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.9645\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.6886\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 1.5336\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4403\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.3727\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3202\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.2749\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2319\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1917\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1497\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1068\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0625\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0145\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.9647\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.9118\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.8603\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.8078\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7574\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.7098\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.6679\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.6298\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.5967\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5684\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5442\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5258\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5078\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4924\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4812\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.4702\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4639\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4552\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4498\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4424\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.4401\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.4359\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4327\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4296\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4276\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4269\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4265\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 55ms/step - loss: 0.4260\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4218\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4178\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4172\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4160\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4226\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4286\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.4290\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.4316\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a51dc8700>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvFHz7-KT9-D",
        "outputId": "acf79805-fdf2-4c89-9386-e6c9d39b6398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"custom_training_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('poem_eng1.h5py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xQCUnZuUC11",
        "outputId": "e8f12426-4ef0-495f-f461-aed776c2449e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRi87BK2HGre",
        "outputId": "f02a3bb1-7491-45f0-be4f-84bcd5ceab93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.3767\n",
            "Epoch 1 Batch 50 Loss 0.3980\n",
            "Epoch 1 Batch 100 Loss 0.4360\n",
            "Epoch 1 Batch 150 Loss 0.4598\n",
            "\n",
            "Epoch 1 Loss: 0.4266\n",
            "Time taken for 1 epoch 14.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 0.3742\n",
            "Epoch 2 Batch 50 Loss 0.3967\n",
            "Epoch 2 Batch 100 Loss 0.4345\n",
            "Epoch 2 Batch 150 Loss 0.4603\n",
            "\n",
            "Epoch 2 Loss: 0.4297\n",
            "Time taken for 1 epoch 13.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 0.3965\n",
            "Epoch 3 Batch 50 Loss 0.4135\n",
            "Epoch 3 Batch 100 Loss 0.4288\n",
            "Epoch 3 Batch 150 Loss 0.4823\n",
            "\n",
            "Epoch 3 Loss: 0.4269\n",
            "Time taken for 1 epoch 12.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 0.3859\n",
            "Epoch 4 Batch 50 Loss 0.4079\n",
            "Epoch 4 Batch 100 Loss 0.4250\n",
            "Epoch 4 Batch 150 Loss 0.4754\n",
            "\n",
            "Epoch 4 Loss: 0.4263\n",
            "Time taken for 1 epoch 11.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 0.3879\n",
            "Epoch 5 Batch 50 Loss 0.3816\n",
            "Epoch 5 Batch 100 Loss 0.4353\n",
            "Epoch 5 Batch 150 Loss 0.4993\n",
            "\n",
            "Epoch 5 Loss: 0.4324\n",
            "Time taken for 1 epoch 11.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 0.3874\n",
            "Epoch 6 Batch 50 Loss 0.4072\n",
            "Epoch 6 Batch 100 Loss 0.4213\n",
            "Epoch 6 Batch 150 Loss 0.4696\n",
            "\n",
            "Epoch 6 Loss: 0.4302\n",
            "Time taken for 1 epoch 11.24 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 0.3881\n",
            "Epoch 7 Batch 50 Loss 0.4127\n",
            "Epoch 7 Batch 100 Loss 0.4398\n",
            "Epoch 7 Batch 150 Loss 0.4753\n",
            "\n",
            "Epoch 7 Loss: 0.4337\n",
            "Time taken for 1 epoch 11.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 0.3871\n",
            "Epoch 8 Batch 50 Loss 0.4158\n",
            "Epoch 8 Batch 100 Loss 0.4534\n",
            "Epoch 8 Batch 150 Loss 0.4719\n",
            "\n",
            "Epoch 8 Loss: 0.4340\n",
            "Time taken for 1 epoch 11.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 0.3818\n",
            "Epoch 9 Batch 50 Loss 0.4092\n",
            "Epoch 9 Batch 100 Loss 0.4245\n",
            "Epoch 9 Batch 150 Loss 0.4773\n",
            "\n",
            "Epoch 9 Loss: 0.4348\n",
            "Time taken for 1 epoch 12.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 0.3948\n",
            "Epoch 10 Batch 50 Loss 0.4053\n",
            "Epoch 10 Batch 100 Loss 0.4494\n",
            "Epoch 10 Batch 150 Loss 0.4698\n",
            "\n",
            "Epoch 10 Loss: 0.4382\n",
            "Time taken for 1 epoch 12.87 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xaiTMxYAHJ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0JEpjpKGZo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}